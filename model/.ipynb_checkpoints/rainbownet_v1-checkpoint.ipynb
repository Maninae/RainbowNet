{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5hTe35ToN7tF"
   },
   "outputs": [],
   "source": [
    "# Sources\n",
    "# --------\n",
    "# Emil Wallner blog post and code on Floydhub for image colorization\n",
    "# \"Deep Koalarization\": https://github.com/baldassarreFe/deep-koalarization\n",
    "# \"Colorful Image Colorization\": https://github.com/richzhang/colorization\n",
    "#Todo: Remove this line once it is installed, reset the kernel: Menu > Kernel > Reset & Clear Output\n",
    "# !git clone https://github.com/fchollet/keras.git && cd keras && python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271,
     "status": "error",
     "timestamp": 1510812387125,
     "user": {
      "displayName": "Cindy Lin",
      "photoUrl": "//lh6.googleusercontent.com/-QZtXpumhTbo/AAAAAAAAAAI/AAAAAAAAMg8/iIMleBruZm0/s50-c-k-no/photo.jpg",
      "userId": "102365799180351563010"
     },
     "user_tz": 480
    },
    "id": "ljuCQQRi_0A_",
    "outputId": "c3e7f92f-bcbd-4cad-98af-3cef58a46f84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import matplotlib\n",
    "# This line was for some of our machines; some edge case\n",
    "# matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "# import keras.backend as K\n",
    "from keras.layers.core import RepeatVector\n",
    "# Not sure we need all of these\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eBcisdtLBovS"
   },
   "outputs": [],
   "source": [
    "# Paths and utilities\n",
    "# -------------------\n",
    "# Assuming this notebook's directory (<repository root>/model) is root.\n",
    "data_dir = \"../data/\"\n",
    "train_dir = data_dir + \"train/\"\n",
    "test_dir = data_dir + \"test/\"\n",
    "results_dir = \"../results/\"\n",
    "\n",
    "model_chkpt = \"model_params/\"\n",
    "model_metadata_dir = \"metadata/\"\n",
    "model_name = \"rainbownet\"\n",
    "\n",
    "\n",
    "# Adjustable constants\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 1\n",
    "GRID_SIZE = 8\n",
    "NUM_TOTAL_BUCKETS = (256//GRID_SIZE)**2\n",
    "NUM_VALID_BUCKETS = 395 # with GRID_SIZE=8\n",
    "\n",
    "# Python 3 peculiarities\n",
    "if sys.version_info[0] == 3:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjbsQR3aK_cU"
   },
   "source": [
    "## Helper Functions\n",
    "Stuff we're using in the other functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9Ha_1ZZqLFXy"
   },
   "outputs": [],
   "source": [
    "def resize_images(images, shape):\n",
    "  \"\"\" Does exactly what it sounds like it does.\n",
    "      |shape|: a 3-tuple of the dimensions to resize to. \n",
    "               (H, W, 3) usually.\n",
    "  \"\"\"\n",
    "  resized = []\n",
    "  for i in images:\n",
    "    im = resize(im, shape, mode='constant')\n",
    "    resized.append(im)\n",
    "  resized = np.array(resized)\n",
    "  return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(obj, name, plimit=500):\n",
    "    typ = type(obj)\n",
    "    typstr = str(typ)\n",
    "    \n",
    "    # for debugging ndarray shape (large use case)\n",
    "    shape = obj.shape if typ == type(np.array([0])) else \"(N/A)\"\n",
    "    shapestr = str(shape)\n",
    "\n",
    "    objstr = str(obj)\n",
    "    objout = objstr if len(objstr) < plimit else objstr[:plimit] + \"\\n[... %s output truncated]\" % name\n",
    "    \n",
    "    outputs = [typstr, shapestr, name + \":\\n\", objout]\n",
    "    print \" \".join(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfgjGUVkkqEx"
   },
   "source": [
    "## **The Inception ResNet Fusion**\n",
    "\n",
    "We're going to be using Google's Inception ResNet v2 to augment our CNN. This model has been highly trained on millions of examples, and can detect features that would be useful in colorizing our test images. The fusion of IRNet features with our CNN will come later in RainbowNetModel(). For now, these functions load the model and set up predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mRJoYEaRAjkl"
   },
   "outputs": [],
   "source": [
    "def load_inception_net():\n",
    "  inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "  inception.load_weights(data_dir + 'inception_resnet_v2_weights.h5')\n",
    "  inception.graph = tf.get_default_graph()\n",
    "  return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1fh3CRW0EY-p"
   },
   "outputs": [],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "  \"\"\" One forward pass through the InceptionResNet for one image,\n",
    "      to give said image a 1000-vector embedding of it's features.\n",
    "      \n",
    "      |grayscaled_rgb|: the images we are embedding in grayscale. \n",
    "      \n",
    "      :return: (None, 1000) vector.\n",
    "  \"\"\"\n",
    "  # Resize all the images to (299, 299, 3).\n",
    "  grayscaled_rgb_resized = resize_images(grayscaled_rgb, (299, 299, 3))\n",
    "\n",
    "  # preprocessing for the Inception resnet\n",
    "  grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "\n",
    "  # Predict.\n",
    "  with inception.graph.as_default():\n",
    "    embed = inception.predict(grayscaled_rgb_resized)\n",
    "  \n",
    "  return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISwY0HA1lsh2"
   },
   "source": [
    "## Finding our color buckets\n",
    "This is how we discretized the *ab* channels' color space into buckets for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WyvXuOITeoT0"
   },
   "outputs": [],
   "source": [
    "def discretize_ab_space():\n",
    "    # initialize a blank array of (# buckets, 2).\n",
    "    space = np.zeros(((256//GRID_SIZE)**2, 2))\n",
    "    \n",
    "    bucket_index = 0\n",
    "    for a in xrange(-128, 128, GRID_SIZE):\n",
    "        for b in xrange(-128, 128, GRID_SIZE):\n",
    "            a_mid = float(a + GRID_SIZE / 2)\n",
    "            b_mid = float(b + GRID_SIZE / 2)\n",
    "            space[bucket_index] = a_mid, b_mid\n",
    "            bucket_index += 1\n",
    "\n",
    "    return space\n",
    "\n",
    "def compute_bucket_maps(bucket2ab_map=None, ab2bucket_map=None):\n",
    "    \n",
    "    # 1) Get our monte carlo ab color space.\n",
    "    # Note: skimage.color's rgb2lab takes in RGB values normalized to [0,1].\n",
    "    # Step sizes of 3 for R,G,B is granular enough for our purposes,\n",
    "    #   in sampling RGB space.\n",
    "    all_rgb_normed = [[r/255., g/255., b/255.] for r in xrange(0,256,3) \\\n",
    "                                               for g in xrange(0,256,3) \\\n",
    "                                               for b in xrange(0,256,3)]\n",
    "    NUM_RGB_SAMPLES = len(all_rgb_normed)\n",
    "    all_rgb_normed = np.array(all_rgb_normed)\n",
    "    all_rgb_normed = all_rgb_normed[:,np.newaxis,:]        # (#rgb colors, 1, 3=rgb)\n",
    "    assert all_rgb_normed.shape == (NUM_RGB_SAMPLES, 1, 3) #  new axis because skimage demands it.\n",
    "    ab_colors = rgb2lab(all_rgb_normed)                    # (#rgb colors, 1, 3=lab)\n",
    "    ab_colors = np.squeeze(ab_colors)[:,1:]                # (#rgb colors, 2=ab)\n",
    "    assert ab_colors.shape == (NUM_RGB_SAMPLES, 2)\n",
    "    \n",
    "    # 2) Get our buckets. \n",
    "    # ab_buckets[bucket #] = (a,b) coord for that bucket, even if it isn't a valid RGB.\n",
    "    ab_buckets_centers = discretize_ab_space()             # (#buckets, 2=ab)\n",
    "    assert ab_buckets_centers.shape == (NUM_TOTAL_BUCKETS, 2)\n",
    "    \n",
    "    # 3) Now we calculate the closest bucket for each ab color, and make the maps.\n",
    "    closest = [np.argmin(np.linalg.norm(ab_buckets_centers - color, axis=1)) for color in ab_colors]\n",
    "    valid_buckets = set(closest)\n",
    "    bucket2ab_map = {bucket : tuple(ab_buckets_centers[bucket]) for bucket in valid_buckets}\n",
    "    # Assert there are no repeats in the map\n",
    "    assert len(bucket2ab_map.values()) == len(set(bucket2ab_map.values()))\n",
    "    ab2bucket_map = {bucket2ab_map[bucket] : bucket for bucket in bucket2ab_map}\n",
    "    assert len(ab2bucket_map.values()) == len(set(ab2bucket_map.values()))\n",
    "    \n",
    "    print (\"There are %d final buckets.\" % len(bucket2ab_map))\n",
    "    print (\"There are %d final colors.\" % len(ab2bucket_map))\n",
    "    \n",
    "    return bucket2ab_map, ab2bucket_map\n",
    "\n",
    "def plot_mapping(bucket2ab_map):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    for bucket in bucket2ab_map:\n",
    "        a_mid = bucket2ab_map[bucket][0]\n",
    "        a_low = a_mid - GRID_SIZE / 2\n",
    "        b_mid = bucket2ab_map[bucket][1]\n",
    "        b_low = b_mid - GRID_SIZE / 2\n",
    "\n",
    "        lab_pixel = np.array([50., a_mid, b_mid])\n",
    "        lab_pixel = np.reshape(lab_pixel, (1, 1, 3))\n",
    "        rgb = tuple(lab2rgb(lab_pixel)[0][0])\n",
    "\n",
    "        ax.add_patch(\n",
    "            patches.Rectangle(\n",
    "                (b_low, -a_low), \n",
    "                GRID_SIZE, \n",
    "                GRID_SIZE, \n",
    "                facecolor=rgb\n",
    "            )\n",
    "        )\n",
    "\n",
    "    plt.xlim([-128, 128])\n",
    "    plt.ylim([-128, 128])\n",
    "    plt.show()\n",
    "    \n",
    "# This function is used in discretize() to turn an entire Lab image into an array\n",
    "#   of bucket labels. See next^2 cell.\n",
    "def get_bucket(a, b, ab2bucket_map):\n",
    "    \"\"\" Given an a,b coordinate in [L]ab color space, return the \n",
    "        closest ab color bucket whose center is closest (as an index).\n",
    "    \"\"\"\n",
    "    point = np.array([[a,b]])\n",
    "    buckets = np.array(ab2bucket_map.keys())   # (NUM_VALID_BUCKETS, 2)\n",
    "    closest_bucket = np.argmin(np.linalg.norm(buckets - point))\n",
    "    return closest_bucket\n",
    "\n",
    "    # e.g. -128 through -119 maps to -123, when GRID_SIZE=10\n",
    "    #a_mid = GRID_SIZE * ((a + 128) // GRID_SIZE) - 128 + GRID_SIZE // 2\n",
    "    #b_mid = GRID_SIZE * ((b + 128) // GRID_SIZE) - 128 + GRID_SIZE // 2\n",
    "\n",
    "    #assert (a_mid, b_mid) in ab2bucket_map\n",
    "    #return ab2bucket_map[(a_mid, b_mid)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "llvS3JwH-8Lo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 395 final buckets.\n",
      "There are 395 final colors.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAD8CAYAAABtq/EAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126cc2f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The script for loading our bucket maps.\n",
    "If there are .pkl's of maps we've stored from a previous computation,\n",
    "    load those.\n",
    "Otherwise, run compute_bucket_maps() to get them, and dump into\n",
    "    pickles for the future.\n",
    "\"\"\"\n",
    "\n",
    "# To open: `map = pickle.load(open(<filename>, 'r'))`\n",
    "BUCKET2AB = model_metadata_dir + 'bucket2ab_map.pkl'\n",
    "AB2BUCKET = model_metadata_dir + 'ab2bucket_map.pkl'\n",
    "\n",
    "bucket2ab_map = None\n",
    "ab2bucket_map = None\n",
    "\n",
    "# No pickles found. Compute the maps.\n",
    "if not os.path.isfile(BUCKET2AB) or not os.path.isfile(AB2BUCKET):\n",
    "  bucket2ab_map, ab2bucket_map = compute_bucket_maps()\n",
    "  plot_mapping(bucket2ab_map)\n",
    "  \n",
    "  pickle.dump(bucket2ab_map, open(BUCKET2AB, 'wb'))\n",
    "  pickle.dump(ab2bucket_map, open(AB2BUCKET, 'wb'))\n",
    "  \n",
    "# We found the pickles! Load them.\n",
    "else:\n",
    "  print(\"Loading buckets from pickle.\")\n",
    "  bucket2ab_map = pickle.load(open(BUCKET2AB, 'rb'))\n",
    "  ab2bucket_map = pickle.load(open(AB2BUCKET, 'rb'))\n",
    "\n",
    "# --------------------------\n",
    "\n",
    "if NUM_VALID_BUCKETS != len(bucket2ab_map):\n",
    "  print(\"NUM_VALID_BUCKETS=%s does not match the number of buckets we computed.\"\n",
    "         % str(NUM_BUCKETS))\n",
    "  print(\"Setting NUM_VALID_BUCKETS=%d\" % len(bucket2ab_map))\n",
    "  NUM_VALID_BUCKETS = len(bucket2ab_map)\n",
    "\n",
    "print(\"Buckets loaded successfully!\")\n",
    "\n",
    "# bucket2ab_map: map of bucket # to (a, b)\n",
    "# ab2bucket_map: map of (a, b) to bucket #\n",
    "# use get_bucket(lab_pixel, ab2bucketmap) to get corresponding (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing for images\n",
    "The following two functions are for turning an *Lab* image(s) into an array of bucket labels, and the converse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_-1k44vKGtu3"
   },
   "outputs": [],
   "source": [
    "def discretize(images_ab):\n",
    "  \"\"\" This is a preprocessing step, that will be used in converting \n",
    "      Y (ab image true labels) into buckets, so that we can calculate\n",
    "      a loss in colorization_loss().\n",
    "  \n",
    "      |images_ab|: (m, H, W, 2) array representing ab channels of images.\n",
    "  \n",
    "      :return: (m, H, W) array where each entry is in [0, NUM_BUCKETS].\n",
    "               One of the NUM_BUCKETS=259 color buckets we found.\n",
    "  \"\"\"\n",
    "  if len(images_ab.shape) == 3:\n",
    "    images_ab = images_ab[np.newaxis,:,:,:]\n",
    "    \n",
    "  m, H, W, _ = images_ab.shape\n",
    "  images_d = np.zeros((m, H, W))\n",
    "\n",
    "  for i in xrange(m):\n",
    "    for h in xrange(H):\n",
    "      for w in xrange(W):\n",
    "        (a,b) = images_ab[i,h,w]\n",
    "        images_d[i,h,w] = get_bucket(a, b, ab2bucket_map)\n",
    "  \n",
    "  return images_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZvamQXbWhFYz"
   },
   "outputs": [],
   "source": [
    "# This function will be called when we want to demo a predicted image.\n",
    "# It takes an image with pixels labeled with buckets, and transforms it\n",
    "#   into 2 color channels.\n",
    "\n",
    "def inverse_discretize(images_d):\n",
    "  \"\"\" The inverse of the above function. Maps the indicated bucket to\n",
    "      the mean of that bucket.\n",
    "      \n",
    "      |images_d|: an array (m,H,W) with the color bucket assigned to each\n",
    "                  pixel.\n",
    "                  \n",
    "      :return: (m, H, W, 2) array with ab color values.\"\"\"\n",
    "  if len(images_d.shape) == 2:\n",
    "    images_d = images_d[np.newaxis,:,:]\n",
    "\n",
    "  m, H, W = images_d.shape\n",
    "  images_ab = np.zeros((m, H, W, 2))\n",
    "  \n",
    "  for i in xrange(m):\n",
    "    for h in xrange(H):\n",
    "      for w in xrange(W):\n",
    "        bucket = images_d[i,h,w]\n",
    "        images_ab[i,h,w] = bucket2ab_map[bucket] # Sets to [a,b]\n",
    "  \n",
    "  return images_ab\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NASk5-4fv6wy"
   },
   "source": [
    "## Getting the data\n",
    "This function loads the data, for training and for test.\n",
    "\n",
    "```preprocess_data()``` then processes them for input into our CNN. This involves converting to *Lab* and separating the channels, running through InceptionResnet, and normalizing the *L* channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "gjrKq1-pCWC2"
   },
   "outputs": [],
   "source": [
    "# Examples of use:\n",
    "# load_data(train_dir)\n",
    "# load_data(test_dir)\n",
    "\n",
    "def load_data(directory):\n",
    "    \"\"\" Load an entire set of |m| examples. If loading entire dataset takes\n",
    "      too much memory, may have to run in batches: \n",
    "      train, save chkpt.\n",
    "      Put new examples in the directory.\n",
    "      Repeat. \n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(directory):\n",
    "      image = load_img(directory + filename)  # PIL image\n",
    "      images.append(img_to_array(image))      # np.array\n",
    "\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "fjFTS5AOQPYW"
   },
   "outputs": [],
   "source": [
    "# initialize inception resnet\n",
    "inception = load_inception_net()\n",
    "\n",
    "def preprocess_data(images):\n",
    "    \"\"\" Preprocess the data for input into our rainbownet model.\n",
    "    \"\"\"\n",
    "    # Resize them to (224,224)\n",
    "    images_resized = resize_images(images, (224, 224, 3))\n",
    "\n",
    "    # Get the inception embeddings from grayscaled images. \n",
    "    #   This will be part of our passed in input.\n",
    "    grayscaled_images = gray2rgb_vec(rgb2gray_vec(images_resized))\n",
    "    embs = create_inception_embedding(grayscaled_images) # already vec'd\n",
    "\n",
    "    rgb2gray_vec = np.vectorize(rgb2gray)\n",
    "    gray2rgb_vec = np.vectorize(gray2rgb)  \n",
    "    rgb2lab_vec = np.vectorize(rgb2lab)\n",
    "\n",
    "    # Separate the l- and ab- channels\n",
    "    images_lab = rgb2lab_vec(images)\n",
    "    images_l = images_lab[:,:,:,0]        # first channel is L\n",
    "    images_ab = images_lab[:,:,:,1:]      # second 2, ab channels\n",
    "\n",
    "    # Normalize L channel to be between 0, 1.\n",
    "    bottom = np.amin(images_l, axis=(1,2), keepdims=True)\n",
    "    images_l -= bottom\n",
    "    top = np.amax(images_l, axis=(1,2), keepdims=True)\n",
    "    images_l /= top\n",
    "\n",
    "    # Create X, composed of L channel + the embedding\n",
    "    X = zip(images_l, embs)         # tuple: (m, H, W), (m, 1000)\n",
    "\n",
    "    # Create Y, including discretizing the ab image\n",
    "    Y = discretize(images_ab)    # shape (m, H, W)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sK7ROlEzwCaz"
   },
   "source": [
    "## Defining our loss\n",
    "Our colorization loss is the softmax cross-entropy between the multinomial color distributions of every pixel in ```y_true``` and ```y_pred``` over all pixels (H,W), over all images in the minibatch. Inspired by the colorization paper, to solve issues other losses like MSE face.\n",
    "\n",
    "TODO: implement color weighting, like in Richard Zhang et al.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "kagX1JnwGF2o"
   },
   "outputs": [],
   "source": [
    "def colorization_loss(y_true, y_pred):\n",
    "  \"\"\" |y_true|: Our true colors. An array (batch_size, H, W) with entries \n",
    "                specifying one of the buckets that pixel's color is in.\n",
    "      |y_pred|: A (batch_size, H, W, 259) volume with last dimension a\n",
    "                softmax over bucket probabilities.\n",
    "  \n",
    "      This loss involves computing the softmax cross-entropy over pixel's\n",
    "        predicted color bucket, over all images in the batch.\n",
    "  \"\"\"\n",
    "  # softmax cse with logits for each pixel\n",
    "  \n",
    "  # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten\n",
    "  y_true_flat = tf.contrib.layers.flatten(y_true)\n",
    "  y_pred_flat = tf.contrib.layers.flatten(y_pred)\n",
    "  # Turns into one-hot representation\n",
    "  y_true_cat = to_categorical(y_true_flat, num_classes=NUM_BUCKETS)\n",
    "  \n",
    "  return categorical_crossentropy(y_true_cat, y_pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO3Vi_Mey7R-"
   },
   "source": [
    "## Defining our model\n",
    "We initialize the first four layers of our CNN to the VGG16's pretrained layers, and freeze them. This transfer learning will help us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "GwG4zlyDBXE0"
   },
   "outputs": [],
   "source": [
    "def RainbowNetModel():\n",
    "  # THE MEAT OF THE CODE\n",
    "  \n",
    "  embed_input = Input(shape=(1000,))\n",
    "  encoder_input = Input(shape=(224, 224, 1,)) # SHAPE WILL CHANGE, probably\n",
    "\n",
    "  \n",
    "  \"\"\"\n",
    "  #Encoder\n",
    "  \n",
    "  encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "  encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "  encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "  encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "  encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "  encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "  encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "  encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "  #Fusion\n",
    "  fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "  fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "  fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "  fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "  #Decoder\n",
    "  decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "  decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "  decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "  decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "  decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "  decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "  decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "  decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "  \"\"\"\n",
    "  \n",
    "  model = Model(inputs=[encoder_input, embedding_input], outputs=decoder_output)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "D9faritUCfc3"
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "  model_json = model.to_json()\n",
    "  with open(model_chkpt + model_name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "  model.save_weights(model_chkpt + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AUNILyqwO1LF"
   },
   "outputs": [],
   "source": [
    "def load_existing_model():\n",
    "  weights_path = model_chkpt + model_name + \".h5\"\n",
    "\n",
    "  if not os.path.isfile(weights_path):\n",
    "    print(\"The model at path %s was not found.\" % weights_path)\n",
    "    quit()\n",
    "    \n",
    "  model = RainbowNetModel()\n",
    "  rainbowModel.load_weights(weights_path)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iMohPPAmMqL"
   },
   "source": [
    "## Time to train the model!\n",
    "Now we can run our model. It will save its parameters after every training session.\n",
    "\n",
    "If you're looking to only predict, run the cell after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JlWI0B-DEZHo"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-936700860b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e65b0f9af2b1>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \"\"\"\n\u001b[1;32m     12\u001b[0m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# PIL image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train/'"
     ]
    }
   ],
   "source": [
    "images_train = load_data(train_dir)\n",
    "X_train, Y_train = preprocess_data(images_train)\n",
    "\n",
    "images_test = load_data(test_dir)\n",
    "X_test, Y_test = preprocess_data(images_test)\n",
    "\n",
    "# Run the model!\n",
    "rainbowModel = RainbowNetModel()\n",
    "\n",
    "rainbowModel.compile(optimizer='adam', \n",
    "                     loss=colorization_loss,\n",
    "                     metrics=['top_k_categorical_accuracy'])\n",
    "\n",
    "rainbowModel.fit(x=X_train, y=Y_train,\n",
    "                 epochs=N_EPOCHS,\n",
    "                 batch_size=BATCH_SIZE)\n",
    "\n",
    "save_model(rainbowModel)\n",
    "\n",
    "# Get predictions after we've trained.\n",
    "predictions = rainbowModel.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOcWNQ9SOZWV"
   },
   "source": [
    "## Predict using the model\n",
    "If you don't want to train, but only use the saved model to predict something, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zxW_idLSNl8F"
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = load_data(test_dir)\n",
    "\n",
    "rainbowModel = load_existing_model()\n",
    "\n",
    "predictions = rainbowModel.evaluate(X_test, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "rainbownet_v1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
